{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import csv\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.applications import ResNet152\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0' # The second\n",
    "tf.config.set_soft_device_placement(True)\n",
    "\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = './dataset'\n",
    "csv_file = 'newModel_log.csv'    \n",
    "h5_file = 'newModel_weight.h5'  \n",
    "load_file = 'newModel_weight.h5'  \n",
    "acc_file = 'newModel_acc.png'   \n",
    "loss_file = 'newModel_loss.png'  \n",
    "\n",
    "image_size = 256\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(folder_name)\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_count = len(list(data_dir.glob('images_arranged91/train/*/*.jpg')))\n",
    "vali_img_count = len(list(data_dir.glob('images_arranged91/vali/*/*.jpg')))\n",
    "test_img_count = len(list(data_dir.glob('test/*.jpg')))\n",
    "unlabel_img_count = len(list(data_dir.glob('unlabeled_data/*.jpg')))\n",
    "print(f'training data size: {train_img_count}')\n",
    "print(f'validation data size: {vali_img_count}')\n",
    "print(f'testing data size: {test_img_count}')\n",
    "print(f'unlabel data size: {unlabel_img_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_PosixPath = list(data_dir.glob('images_arranged91/train/*/*.jpg'))\n",
    "vali_img_PosixPath = list(data_dir.glob('images_arranged91/vali/*/*.jpg'))\n",
    "test_img_PosixPath = list(data_dir.glob('test/*.jpg'))\n",
    "unlabel_img_PosixPath = list(data_dir.glob('unlabeled_data/*.jpg'))\n",
    "print(len(train_img_PosixPath),train_img_PosixPath)\n",
    "print(len(vali_img_PosixPath),vali_img_PosixPath)\n",
    "print(len(test_img_PosixPath),test_img_PosixPath)\n",
    "print(len(unlabel_img_PosixPath),unlabel_img_PosixPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels of training images\n",
    "train_img_name = []\n",
    "train_img = []\n",
    "train_label = []\n",
    "vali_img_name = []\n",
    "vali_img = []\n",
    "vali_label = []\n",
    "test_img_name = []\n",
    "test_img = []\n",
    "# unlabel_img_name = []\n",
    "# unlabel_img = []\n",
    "\n",
    "for i in range(len(train_img_PosixPath)):\n",
    "    train_img_name.append(str(train_img_PosixPath[i]).split('/')[4])\n",
    "    train_img.append(img_to_array(load_img(train_img_PosixPath[i], target_size=(image_size,image_size)))/255.)\n",
    "    train_label.append(str(train_img_PosixPath[i]).split('/')[3])\n",
    "print('Training data done.')\n",
    "\n",
    "for i in range(len(vali_img_PosixPath)):\n",
    "    vali_img_name.append(str(vali_img_PosixPath[i]).split('/')[4])\n",
    "    vali_img.append(img_to_array(load_img(vali_img_PosixPath[i], target_size=(image_size,image_size)))/255.)\n",
    "    vali_label.append(str(vali_img_PosixPath[i]).split('/')[3])\n",
    "print('Validation data done.')\n",
    "\n",
    "for i in range(len(test_img_PosixPath)):\n",
    "    test_img_name.append(str(test_img_PosixPath[i]).split('/')[2])\n",
    "    test_img.append(img_to_array(load_img(test_img_PosixPath[i], target_size=(image_size,image_size)))/255.)\n",
    "print('Testing data done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 7777\n",
    "np.random.seed(my_seed)\n",
    "np.random.shuffle(train_img_name)\n",
    "np.random.seed(my_seed)\n",
    "np.random.shuffle(train_img)\n",
    "np.random.seed(my_seed)\n",
    "np.random.shuffle(train_label)\n",
    "np.random.seed(my_seed)\n",
    "np.random.shuffle(vali_img_name)\n",
    "np.random.seed(my_seed)\n",
    "np.random.shuffle(vali_img)\n",
    "np.random.seed(my_seed)\n",
    "np.random.shuffle(vali_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_img)\n",
    "vali_train = np.array(vali_img)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the labels\n",
    "train_label=np.array(train_label)\n",
    "vali_label=np.array(vali_label)\n",
    "y_train = tf.keras.utils.to_categorical(train_label, num_classes=4)\n",
    "vali_label = tf.keras.utils.to_categorical(vali_label, num_classes=4)\n",
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(X_train)\n",
    "std = np.std(X_train)\n",
    "\n",
    "X_train = (X_train-mean)/(std)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "train_data = datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "valid_data = datagen.flow(vali_train, vali_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_models.tfkeras import Classifiers\n",
    "Classifiers.models_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16, VGG19, InceptionResNetV2, ResNet152\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.applications.densenet import DenseNet201\n",
    "SeResNeXT101, preprocess_input = Classifiers.get('seresnext101')\n",
    "SeResNet101, preprocess_input = Classifiers.get('seresnet101')\n",
    "Resnext101, preprocess_input = Classifiers.get('resnext101') # not adopted\n",
    "Mobilenetv2, preprocess_input = Classifiers.get('mobilenetv2')\n",
    "Resnet152v2, preprocess_input = Classifiers.get('resnet152v2')\n",
    "Xception, preprocess_input = Classifiers.get('xception')\n",
    "Densenet169, preprocess_input = Classifiers.get('densenet169')\n",
    "\n",
    "\n",
    "\n",
    "base_model = SeResNeXT101(\n",
    "    weights = \"imagenet\", \n",
    "    include_top=False, \n",
    "    input_shape=(256,256, 3))\n",
    "    \n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers in the pre-trained model\n",
    "for layer in base_model.layers[:-7]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new classification layers on top of the pre-trained model\n",
    "# last_layer = 'conv_pw_13_relu'  #mobilenet\n",
    "# last_layer = 'conv_7b_ac' #inceptionResnetv2\n",
    "# last_layer ='mixed10' # inceptionV3\n",
    "# last_layer ='conv5_block3_out' # resnet152\n",
    "# last_layer ='relu' # densenet201\n",
    "last_layer ='activation_165' # SeResNeXT101\n",
    "# last_layer ='block14_sepconv2_act' # Xception\n",
    "# last_layer ='out_relu' # mobilenetV2 \n",
    "# last_layer ='activation_2349' # SeResNeT101 \n",
    "# last_layer ='block5_pool' # vgg19\n",
    "# last_layer ='activation_165' # SeResNeXT101 \n",
    "# last_layer ='post_relu' # Resnet152V2\n",
    "last_layer = base_model.get_layer(last_layer)\n",
    "\n",
    "x = Flatten()(last_layer.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(4, activation='softmax')(x)  # Assuming 4 classes for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model combining the pre-trained model with the new classification layers\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "def lr_scheduler(epoch, lr):\n",
    "    decay_rate = 0.1\n",
    "    decay_step = 10\n",
    "    if epoch % decay_step == 0 and epoch:\n",
    "        return lr * decay_rate\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=h5_file, verbose=1, save_best_only=True, save_weights_only=True)\n",
    "csv_logger = CSVLogger(filename=csv_file, separator=',', append=False)\n",
    "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=30, verbose=1, mode='min')\n",
    "optimizer = Adam(learning_rate=lr)\n",
    "reduce_lr = LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, \n",
    "                        epochs=500,\n",
    "                        verbose=1, \n",
    "                        validation_data=valid_data,\n",
    "                        # class_weight=class_weight,\n",
    "                        callbacks=[checkpoint, csv_logger, earlystopping,reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# data = pd.read_csv(csv_file)\n",
    "# epoch = data[data.columns[0]]\n",
    "# accuracy = data[data.columns[1]]\n",
    "# loss = data[data.columns[2]]\n",
    "# val_accuracy = data[data.columns[3]]\n",
    "# val_loss = data[data.columns[4]]\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "# plt.figure(1)\n",
    "# # \"bo\" is for \"blue dot\"\n",
    "# plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# # b is for \"solid blue line\"\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(2)\n",
    "# acc_values = history.history['accuracy']\n",
    "# val_acc_values = history.history['val_accuracy']\n",
    "# plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(test_img)\n",
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(x_test)\n",
    "std = np.std(x_test)\n",
    "\n",
    "x_test = (x_test-mean)/(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model.load_weights(h5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(x_test)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to CSV file\n",
    "output_file = 'output.csv'\n",
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Name', 'Type'])\n",
    "    for i in range(len(test_img_name)):\n",
    "        writer.writerow([test_img_name[i], y_test[i]])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
